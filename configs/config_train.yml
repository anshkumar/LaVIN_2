fit:
  trainer:
    accelerator: gpu
    strategy: 'ddp_find_unused_parameters_true'
    devices: 2
    num_nodes: 1
    max_epochs: 20
    accumulate_grad_batches: 8
    plugins: 
      - class_path: lightning.pytorch.plugins.BitsandbytesPrecision
        init_args:
          mode: 'nf4'
          ignore_modules: 
            - 'visual_backbone'
            - 'conv_A'
            - 'conv_B'
    callbacks:
      - class_path: lightning.pytorch.callbacks.ModelCheckpoint
        init_args:
          dirpath: "ckpt"
          save_on_train_epoch_end: true

  # ckpt_path: '/home/sort/ved/LaVIN_2/ckpt/epoch=0-step=199.ckpt'
  model:
    class_path: lavin.LightningTransformer
    init_args:
      llama_model_path: ./data/weights/
      llm_model: 7B
      max_seq_len: 512
      val_batch_size: 32
      adapter_dim: 16
      gradient_checkpointing: true
      learning_rate: 0.00225 #0.001125 # base_lr(0.009) * train_batch_size * accumulate_grad_batches * num_nodes * devices / 256
      weight_decay: 0.02
  data:
    class_path: util.datasets.ScienceQADataModule
    init_args:
      num_workers: 24
      problems_path: ./data/problems.json
      pid_splits_path: ./data/pid_splits.json
      captions_path: ./data/captions.json
      images_path: ./data/images/
      tokenizer_path: ./data/weights/tokenizer.model
      max_words: 512
      image_height: 336
      image_width: 336
      train_batch_size: 4
      val_batch_size: 32
      prompt_format: QCM-ALE
      use_caption: false
